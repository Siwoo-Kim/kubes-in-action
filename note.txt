

Cheat sheet 
    kube logs [podName] --previous
    
    kube label pod [podName] [key]=[value] --overwrite
    
    kube get rc [rcName]
    
    kube delete rc [rcName] --cascade=false
    
    kube describe rs
    
    kube exec [podName] -- [command]
    
    kube exec [podName] -it bash
    
    kube exec [podName] env
        - check service's ip & port
        
    kube get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
        - get IPs of all nodes.
        
    openssl genrsa -out tls.key 2048
    
    openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com
    
    kube create secret tls tls-secret --cert=tls.cert --key=tls.key
        - TLS cert & private key creation and the kube's secret 
    
    kube port-forward [podName] [localPort]:[podPort]
        forwarding a port from local machine to the pod.
    
Docker
    - Docker Hub registry
    - Image
    - Container
    - Dockerfile (instruction to build the image)
    
Dockerfile
    FROM
        - base image
    ADD
        - add file from local to directory in the image
    ENTRYPOINT
        - defining command to execute the app in the container
        
    docker build -t [imgName] .
    
creating, running, and sharing a container image.
    - docker images
        list images
    - docker ps
        list containers
    - docker run --name [containerName] -p [hostPort:containerPort] -d [image:tag]
        run container from image
    - docker inspect [containerName]
        show container info
    - docker exec -it kubia-container [sh|bash]
        interactive mode in container
    - docker stop [containerName]   
    - docker rm [containerName]
        docker rm $(docker ps -a -q)
    
    - docker tag [imgName] [id]/[imgName]
        tagging img
    - docker push [id]/imgName
        push img to registry
        
Minikube
     curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
     sudo install minikube-linux-amd64 /usr/local/bin/minikube
        - install minikube

    minikube start
        - start kubes     
        
    kubectl cluster-info
        - cluster information
    kubectl get nodes
        - list nodes
    kubectl describe node [nodeName]
    
    kube get pods
    kube describe pods
    
    kube expose pods kubia --type=LoadBalancer --name kubia-http
        - LoadBalancer 을 통해 pods 을 외부로 expose
        
        (minikube service kubia-http)
        
    kube get services
    
    kube logs [podName] -c [containerName]
    
    kube port-forward [containerName] [hostPort]:[podPort]
        - pod testing 을 하기 위해 포트포워딩.

Tips
    ~/.bashrc 파일에 추가
    alias kube=kubectl
    
    source <(kubectl completion bash | sed s/kubectl/kube/g)
        - auto completion
        
    kubectl run kubia --image=sm123tt/kubia --port=8080
    
Pod
    - group of containers (one or more tightly related containers in the same Linux namespace)
        - to support IPC (Inter-Process Communication or through locally stored file)
        - binding containers together and provide them with the same env
        - share same hostname and network interfaces.
        - using Volume, each container share the filesystem.
        - every pod can access every other pod at the IP address (flat network)
            
    - has its own IP, hostname, processes.
    - Pods are spread out across different worker nodes.
    - All containers of a pod run on the same node.
    
    Node                Node
    pod1                pod1
    pod2 (10.x.x.x)     pod2
        - container1    ...
        - container2
    pod3
    
    Pod Creation flow
        kubectl -> REST HTTP req -> Kube API Server -> Scheduler -> Node -> Kubelet -> Docker

YAML
    reference
        http://kubernetes.io/docs/api 
        kubectl explain pods
    structure
        kind
           type of object/resource
        metadata
            pod metadata (name, labels, annotations..)
        spec
            list of pod's containers, volumes...
        status
            detail statuf of the pod and containers
            
    kube create -f [yamlFile]
        - creating a pod from YAML or JSON
    
Grouping containers.
    1. do they need to be run together or can they run on different hosts?
    2. do they represent a single whole or are they independent components?
    3. must they be scaled together or individually?
    

Creating pods from YAML
    - creating resources by posting YAML to the kubes API server
    
   Exposing ReplicationController
        - LoadBalancer service
            connect to the pod through the load balancer's public IP
     
unknown
    Deployment 
    
    ReplicationController
        - replicate pods (availability)
    
    Service
        - service has static IP and map request to the pods.
        - client connect to the service through the service.

Grouping pods 
    Labels.
        1. categorizing resources into subsets.
        2. key-value pair you attach to a resource
        3. using label selectors, selecting resources.
        4. resource can have more than one label.
        
        Label 을 Pod 에 attach
        
            apiVersion: v1
            kind: Pod
            metadata:
              name: [podName]
              labels:
                [key]:value
                ..
                
        Label 확인
            kube get pod --show-labels
            kube get pod -L [label1],[label2]
        
        Label 변경
            kube label pods [podName] key=value --overwrite
            
        Label Selector
            kube get pods -l [key]=[value]
            
            selector exp
                'key=value'
                'key!=value'
                'key'
                '!key'
                    pods only doesn't have specified key label
                    
                'key in (val1,val2)'
                    either va1 or val2
            
        nodeSelector:
            key: value
            
        - instruct kube to deploy this pod only to nodes containing the "key" and "value"
        

    Annotations
        doesn't provide query mechanism like query-selector
        can hold much larger pieces of information
        
        apiVersion: ..
        metadata:
            annotations:
                key: |
                {json... }
                
    Namespace
        split components into smaller distinct group (Eg, dev, qa, prod or domain layer)
         
         kube get ns
            - list all namespace in cluster
         kube get pods --namespace [namespaceName]   
         
         apiVersion: v1
         kind: Namespace
         metadata:
            name: [namespaceName]

        - creating namespace
        
        kube create -f [yamlFile] -n [namespaceName]   

Liveness probes
    - periodically execute the probe and restart the container if the probe fails.
    - need to set an initial delay to account for app's start up time (initialDelaySeconds)
    - shouldn't use too many computational resources and shouldn't take too long to complete.
    
    - HTTP GET probe
        prediodically perform HTTP GET request to defermine if the container is still healthy
    - TCP Socket probe
    - Exec probe
    
    apiVersion: v1
    kind: Pod
    metadata:
        name: ..
    spec:
        ...
        livenessProbe:
            httpGet:
                path: [httpPath]
                port: [httpPort]
            initialDelaySeconds: 15
            # will wait 15 seconds before executing the first probe
            
ReplicationControllers
    - ensures its pods are always kept running.
    - makes sure the actual number of "labeled" pods always matched the desired number.
    - if the managed label by rc change, the pod no longer managed.
    - you can edit the rc when rc is running
    
    Components.
        1. label selector
            - determines what pods are in the RC's scope
        2. replica count
            - the desired number of pods
        3. pod template
            - is used when creating new pod replicas.
    
    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: kubia
    spec:
      replicas: 3
      selector:
        app: kubia
      template:
        metadata:
          labels:
            app: kubia
        spec:
          containers:
            - name: kubia
              image: sm123tt/kubia
              ports:
              - containerPort: 8080
              
    kube get rc
        - ger info about rc
    
    kube edit rc kubia
        - change rc's template
        
    kube scale rc kubia --replicas=10
    
ReplicaSet
    - new generation of rc
    - more expressive pod selectors
    - matchExpressions (In, NotIn, Exists, DoesNotExist)
    - AND operation for multiple expressions 
    
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: kubia-rs
    spec:
      replicas: 3
      selector:
          matchExpressions:
            # only difference
            - key: app
              operator: In
              values:
                - ...
                
      template:
        metadata:
          labels:
            app: kubia
        spec:
          containers:
            - name: kubia
              image: sm123tt/kubia
              
    kube get rs
    
Job resource
    - want to run a task that terminates after completing its works. (completable work)
    - If the event of a process failure, the Job can be configured to either restart or not.
    
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: prime-job
    spec:
      template:
        metadata:
          labels:
            app: prime-job
        spec:
          restartPolicy: OnFailure
          #set up policy
          containers:
            - name: main
              image: sm123t/prime 
    
    kube get job
    
Service
    - constant point of entry to a group of pods providing the same service.
    - Each service has an IP address and port that never change while the service exists.
    - use label selectors to specify which pods belong to the same set.
    - "sessionAffinity: ClientIP" allow a certain client to be redirected to the same pod every time.
    - service's IP address and port number will be available in container's env with their name
    
    Exposing pods to other pods in the cluster.
    
        apiVersion: v1
        kind: Service
        metadata:
          name: [svcName]
        spec:
          ports:
            - name: [namedPort]
              port: [svcPort]
              targetPort: [namedPort in Pod]
          selector:
            [key]: [value]
        
        NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
        kubia-svc    ClusterIP      10.102.78.247   <none>        80/TCP           4s

        access through service's http://Cluster-IP/PORT
        access through service's http://[svcName]
        access through service's http://[svcName].[namespace].[svc.cluster.local]
        
    Service Discovery
        kube exec [containerName] env
    

    Exposing service to external clients.
        a few ways to make a service accessible externally.
            1. Setting the service type to NodePort
            2. Setting the service type to LoadBalancer
            3. Creating an Ingress resource.
      
        NodePort service.
            - the service is accessible through the IP address of any cluster node (with their own ip address).
            
            apiVersion: v1
            kind: Service
            metadata:
              name: kubia-svc-nodeport
            spec:
              ports:
                type: NodePort
                - name: http
                  port: 80
                  # port of the service's internal cluster IP
                  targetPort: http
                  # target port of the pods
                  nodePort: 30123
                  # the service will be accessible through the node port with each node's IP
              selector:
                [key]: [value]
                
        
        LoadBalancer 
            - is a NodePort service with an additional infrastructure-provided load balancer. (can access pod via Node's IP and specified port)
            - have its own unique, publicly accessible IP address and will redirect all connections to the service.
            - IP address will be listed as the external IP address.
    
    Session affinity ("sessionAffinity")
        - users will always hit the same pod until the connection is closed.
            (when a connection - keep-alive to a service is opened, all packets belonging to that connection are sent to the single pod)
            
    Removing extra network hop ("externalTrafficPolicy")
        - prevent additional hop by configuring the service to redirect external traffic only to pods running on the node.
        - "Local" external traffic policy may lead to "uneven" distribution of requests
        
    Ingress Resource (GATEWAY)
        - operate at the application layer (HTTP) and provide features such as cookie-based session affinity.
        - Ingress controller should be running in your cluster.
        - make sure all HTTP requests will be sent to the "service"
        - can map "multiple paths" "on the same host" to different services.
        - support HTTPS (TLS connection). attach a certification and a private key to the Ingress
        
        minikube addons enable ingress
            - enable the ingress add-on
        
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        metadata:
          name: [ingName]
        spec:
          tls:
            - hosts:
                - [hostName for TLS]
              secretName: [secretForTLS]
          rules:
            - host: [hostName]
              http:
                paths:
                  - path: [pathForSvc]
                    backend:
                      serviceName: [svcName]
                      servicePort: [svc's Port]
                  
        Ingress Flow
            1. client perform "DNS lookup" of [hostname]
            2. client get IP of the "Ing Controller"
            3. sent HTTP request to "Ing Controller"
            4. "Ing controller" look up pod IPs through the "Endpoints object" with associated the svc (defined in the yaml)
            5. send request to one of the pods
            
Readiness probes.
    - "Liveness probes" keep pods healthy by killing of unhealthy one.
    - Whereas readiness probes just make sure that only pods are ready to request receive them.
    - "Readiness probes" makes sure clients only talk to those healthy pods. (LB healthy checking feature only)
    - Always define a readiness probe.
    
    exec probe
        - a process is executed and the container's status is determined by exit status code.
    HTTP get probe
    TCP Socket probe
        - opens TCP connection to a port of the container. the container's status is determined by result of establishing conection.
     
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: kubia-rs
    spec:
      replicas: 3
      selector:
        matchExpressions:
          - key: app
            operator: In
            ...
      template:
        ...
        spec:
          containers:
            - name: kubia
              ...
              readinessProbe:
                exec:
                  command:
                    - ls
                    - /var/ready

Volumes
    - volume is available to all containers in the pod, must be mounted in each container (VolumeMount in container's spec).
    - can mount the volume in any location of its filesystem
    - new container can see all the files that were written to the volume by the previous container
    - 볼륨은 팟의 컨테이너에게 공유되는 파일시스템, 팟 사이에서 공유되는 파일시스템 두 가지로 나눠지는 듯.
    
    Type of volumes
        emptyDir    - simple empty directory used for storing tansient data
        hostPath    - used for mounting dirs from the node's filesystem into the pod.
        gitRepo     - a volume initialized by checking out the contents of a git repo.
        nfs         - an nfs shared mounted into the pod.
        awsElasticBlockStore, azureDisk - Cloud based volume disk.
        configMap, secret   - special types of volumes used to expose certain kube's resources and cluster info (metadata).

    Sharing data between "containers" in a pod
    
        emptyDir volume
            - volume's lifetime is tied to the pod. (not persistent)
            - volume's contents are lost when the pod is deleted.        
            - used for sharing files between containers running in the same pod.
            - used by a container for when a container needs to write data to disk temporarily.
            
        apiVersion: v1
        kind: Pod
        metadata:
            - name: [podName]
        spec:
            containers:
                - name: [ctName]
                  image: [imgName]
                  ports: 
                    - containerPort: 80
                           protocol: TCP
                  volumeMounts:
                    - mountPath: [mountPath]
                      name: [volName]
                      readOnly: [boolean]
                - name: [ctName]
                ...
                  volumeMounts:
                    - mountPath: [mountPath]
                      name: [volName]
                      ...
            volumes:
             - name: [volName]
               emptyDir: {}
               
    Accessing files on the node's filesystem.
        - hostPath allow pods to access the node's devices.
        
        hostPath volume
            - persistent storage (not deleted when a pod is torn down)
            - never use it to persist data across pods (eg. DB)
            
    Using PERSISTENT Storage
        - the data needs to be accessible from any cluster node. 
        - NAS (network-attached storage)
        
        1. creating disk.
        2. create db pod with the volumes 
        
        apiVersion: v1
        kind: Pod
        metadata:
          name: [podName]
        spec:
          containers:
            - name: [dpContainerName]
              image: [dbImg]
              volumeMounts:
                - mountPath: [mountPath]
                  name: [volName]
              ports:
                - containerPort: [dbPort]
                  protocol: [dpProtocol]
          volumes:
            - name: [volName]
            gcePersistentDisk:
                pdName: [podName]
                fsType: [fileSystemType=ext4]
                
    PV & PVC (Persistent Volume & Persistent Volume Claim)
        - developer doesn't need to know actual network storage infra.
        - PV need to be registered in the kube
        - user request a volume through submitting PVC and the volume will be bound.
        - Need to specify PV's "capacity" and "access mode", "location of disk" and "type of disk"
        - PV is "cluster-level" resources. (available in all nodes)
        - When PVC is created, appropriate PV will binds it to the PVC
        - PVC requires access modes. 
         
         Access Mode (Access Mode is based on nodes)
            RWO - Read Write Once   - Only a single node can mount the volume for reading and writing
            ROX - Read Only Many    - Multiple nodes can mount the volume for reading
            RWX - Read Write Many   - Multiple nodes can mount the volume for read & write
            
         apiVersion: v1
         kind: PersistentVolume
         metadata:
            ...
         spec:
            capacity:
                storage: [sizeOfStorage]
            accessModes:
            - [ReadWriteOnce|ReadOnlyMany|ReadWriteMany]
            persistentVolumeReclaimPolicy: [Retain..]
            [azureDisk|gcePersistentDisk]:
                ...
                
Configuring containerized app.
    - passing command-line args to containers
    - setting customer env variables for each container
    - mounting configuration files through "a specity type of volume".
    
    Defining the command and arg in Docker
        ENTRYPOINT 
            defines the executable invoked when the container is started.
        CMD
            specifies the args that get passed to the ENTRYPOINT
            메인 명령어 (ENTRYPOINT) 와 함께 사용하며 추가 default 인자값을 정의할 때 사용.
            
        ENTRYPOINT ["xxx.sh"]
        CMD ["defaultArg", ..]
        
    Overriding "ENTRYPOINT" and "CMD" in Kubernetes
        ENTRYPOINT - COMMAND
            executable that's executed inside the container
        CMD - ARGS 
            the arguments passed to the executable
    
    Setting ENV variables for a container.
        - 'pod level' env variable
        - 'container level' env variable
        - "env" in container spec
        - can reference previously defined env variable using $(VAR) syntax.
        
        apiVersion: v1
        kind: Pod
        metadata:
            ...
        spec:
            containers:
                - name: ..
                  image: ...
                env:
                  - name: [env]
                   value: [value]
                  - name: ... 
        
    Decoupling config with a ConfigMap
        - decouple the config from the pod descriptor.
        - a map containing key/value pairs with the values ranging from literals to files.
        - use a different ConfigMap in each env
        - "configMapKeyRef.optional" property allow the container starts even if ConfigMap doesn't exist.
        - can pass the values from configmap to pod using "valueFrom", "envFrom"
        - when configmap is "edit"ed, the value in the container also will be updated automatically.
        
        kube create configmap [name] --from-literal=[key]=[value] --from-literal...
        kube get configmap [name] -o yaml
        
        kubectl create configmap [name] --from-file=[/path/to/dir]
            - from files
        kubectl create configmap [name] --from-literal=[key]=[value]
            - from args
        kubectl create -f [yamlFile]
            - from yaml
        
        
        apiVersion: v1
        kind: Pod
        ...
        spec:
            env:
            - name: [VAR_NAME]
              valueFrom:
                configMapKeyRef:
                    name: [configmapName]
                    key: [keyInConfigMap]
                    optional: [true|false]
                    
        apiVersion: v1
        kind: Pod
        ...
        spec:
            envFrom:
            - prefix: [prefix]
              configMapRef:
                name: [configName]   
                
        Passing a ConfigMap as a command-line argument
            - init an env variable from configmap and then refer the variable into arguments
            - using $(VAR_NAME) syntax in "args"
            
            apiVersion: v1
            kind: Pod
            metadata:
                name: [podName]
            spec:
                containers:
                    - image: [imgName]
                      name: [podName]
                      envFrom:
                      - prefix: [prefixOfConfigmap]
                        configMapRef:
                            name: [configName]
                     command: [command]
                     args: ["$(VARIABLE)"]
                     
    Secrets.
        - like ConfigMap (key-value pairs)
        - expose secret entries as files in a volume
        - Secrets are always stored in memory and never written to physical storage.
        - etcd (in master node) stores Secrets in encrpted form.
        - entries are shown as Base64-encoded
        
        kube get secrets
            - get all secrets
            
        Default token secret
            - need for secure talk between pods and Kubenetes API
            - default-token secret is mounted into every container
            - /var/run/secrets/kubernetes.io/serviceaccount
            
        HTTPS (TLS)
            - private key needs to be kept secure.
            - "openssl genrsa" & "openssl req" to generate cert and private ke.
            
           kube create secret [generic|tsl] [secretName] --from-file=...
           
Downward API
    - pass metadata (the pod's data) about hte pod and its env variables or files
    - resource limit or requests is available through "resourceFieldRef"
    - only "volume" mechanism allow to access pods' labels & annotations
    
    Exposing metadata through env.
        - defined in spec.containers.env
        - "valueFrom.fieldRef" or "valueFrom.resourceFieldRef"
        - metadata.[name|namespace], status.potIP, spec.nodeName
        
        apiVersion: v1
        kind: Pod
        metadata:
          name: downard-pod
        spec:
          containers:
            - name: main
              image: busybox
              command: ["sleep", "9999999"]
              resources:
                requests:
                  cpu: 15m
                  memory: 4Mi
              env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                - name: SERIVCE_ACCOUNT
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.serviceAccountName
                - name: CONTAINER_CPU_REQUEST_MILLICORES
                  valueFrom:
                    resourceFieldRef:
                      resource: requests.cpu
                      divisor: 1m
                - name: CONTAINER_MEMORY_LIMIT_KIIBIBYTES
                  valueFrom:
                    resourceFieldRef:
                      resource: limits.memory
                      divisor: 1Ki
                      
        kube exec [podName] -- env
        
    Exposing metadata through volume
        apiVersion: v1
        kind: Pod
        metadata:
          name: downward-vol-pod
          labels:
            foo: bar
          annotations:
            key1: value1
            key2: |
              multi
              line
              value
        spec:
          containers:
            - name: main
              image: busybox
              command: ["sleep", "9999999"]
              resources:
                requests:
                  cpu: 15m
                  memory: 10Mi
                limits:
                  cpu: 100m
                  memory: 15Mi
              volumeMounts:
                - mountPath: /etc/downward
                  name: downward
          volumes:
            - name: downward
              downwardAPI:
                items:
                  - path: "podName"
                    fieldRef:
                      fieldPath: metadata.name
                  - path: "podNamespace"
                    fieldRef:
                      fieldPath: metadata.namespace
                  - path: "labels"
                    fieldRef:
                      fieldPath: metadata.labels
                  - path: "annotations"
                    fieldRef:
                      fieldPath: metadata.annotations
                  - path: "containerCpuRequestMilliCores"
                    resourceFieldRef:
                      containerName: main
                      resource: requests.cpu
                      divisor: 1m
                  - path: "containerMemoryLimitBytes"
                    resourceFieldRef:
                      containerName: main
                      resource: limits.memory
                      divisor: 1Ki

Talking to the kubernetes API server.
    - allow "container" to know more about other pods and other resources defined in the cluster.
    
    kubectl poxy
        - runs a proxy server that accepts HTTP connections on your local machine
        - proxies them to the API server while tacking care of authentication.
        
        kube proxy
            - Starting to serve on 127.0.0.1:8001
        curl 127.0.0.1:8001
            - shows all allowed resources
            - most resource types can be found in "api/v1"
            - batch API group can be found in "apis/batch/v1"
            
        curl 127.0.0.1:8001/apis/
        
    Talking to the API server from within a pod.
        1. location of the API server
        2. Authenticate with the server. ( ca.crt & token )
        
        Kubernetes API HOST & PORT
            - env vars are configured for each service.
            KUBERNETES_SERVICE_HOST
            KUBERNETES_SERVICE_PORT
            KUBERNETES_SERVICE_PORT_HTTP
            
            - also each service gets a DNS entry.
            https://kubernetes
       
        ca crt.
            /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        token
            /var/run/secrets/kubernetes.io/serviceaccount/token
        
        namespace
            /var/run/secrets/kubernetes.io/serviceaccount/namespace
            
    Disabling RBAC (role-based access control)
        
        kube create clusterrolebinding permissive-binding --clusterrole=cluster-admin --group=system:serviceaccounts
            - give all service accounts (including pods) cluster-admin privileges, which allow them to do whatever they want.
    
    Simplifying API server communication.
        - Instead of sending requests to the API server directly, sent them to proxy and proxy take care of auth & encryption & ca verfification.
        
        <!-- https://mvnrepository.com/artifact/io.fabric8/kubernetes-client -->
        <dependency>
            <groupId>io.fabric8</groupId>
            <artifactId>kubernetes-client</artifactId>
            <version>5.4.0</version>
        </dependency>
        
        KubernetesClient client = new DefaultKubernetesClient();
        PodList pods = client.pods().inNamespace("default").list();
        pods.getItems().stream().forEach(System.out::println);